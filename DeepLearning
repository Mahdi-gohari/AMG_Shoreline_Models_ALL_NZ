import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import random
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, Add, Permute, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from tensorflow.keras.regularizers import l2
import optuna
from tensorflow.keras.mixed_precision import set_global_policy
import time

# Custom callback to monitor GPU utilization
class GPUMonitorCallback(Callback):
    def __init__(self, log_frequency=100):
        super(GPUMonitorCallback, self).__init__()
        self.log_frequency = log_frequency
        self.batch_times = []
        self.last_time = None

    def on_train_batch_begin(self, batch, logs=None):
        self.last_time = time.time()

    def on_train_batch_end(self, batch, logs=None):
        if self.last_time is not None:
            batch_time = time.time() - self.last_time
            self.batch_times.append(batch_time)
            
            if batch % self.log_frequency == 0 and batch > 0:
                avg_time = np.mean(self.batch_times[-self.log_frequency:])
                print(f"Batch {batch}: Avg time per batch: {avg_time:.3f}s, "
                      f"Samples/sec: {logs.get('size', 0)/avg_time:.1f}")

    def on_epoch_end(self, epoch, logs=None):
        if self.batch_times:
            avg_time = np.mean(self.batch_times)
            print(f"Epoch {epoch}: Average batch time: {avg_time:.3f}s")

# Aggressive GPU Configuration - CRITICAL FOR GPU UTILIZATION
def configure_gpu():
    """Configure GPU for maximum utilization and eliminate bottlenecks"""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            # Configure all available GPUs
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            tf.config.set_visible_devices(gpus, 'GPU')
            print(f"GPU configured successfully. Available GPUs: {len(gpus)}")
            
            # Enable mixed precision for better GPU utilization
            set_global_policy('mixed_float16')
            
            # Enable XLA JIT compilation for better performance
            tf.config.optimizer.set_jit(True)
            
            # Additional performance optimizations
            tf.config.threading.set_inter_op_parallelism_threads(0)  # Use all CPU cores
            tf.config.threading.set_intra_op_parallelism_threads(0)  # Use all CPU cores
            
        except RuntimeError as e:
            print(f"GPU configuration error: {e}")
    else:
        print("No GPUs found. Running on CPU.")

# Custom learning rate scheduler - optimized
class WarmUpCosineDecay(Callback):
    def __init__(self, initial_lr, warmup_epochs, total_epochs, steps_per_epoch):
        super(WarmUpCosineDecay, self).__init__()
        self.initial_lr = initial_lr
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.steps_per_epoch = steps_per_epoch
        self.global_step = 0
        self.history = {}

    def on_batch_end(self, batch, logs=None):
        self.global_step += 1
        current_epoch = self.global_step // self.steps_per_epoch

        if current_epoch < self.warmup_epochs:
            lr = self.initial_lr * (self.global_step / (self.warmup_epochs * self.steps_per_epoch))
        else:
            progress = (self.global_step - self.warmup_epochs * self.steps_per_epoch) / \
                       ((self.total_epochs - self.warmup_epochs) * self.steps_per_epoch)
            lr = self.initial_lr * 0.5 * (1.0 + np.cos(np.pi * progress))

        self.model.optimizer.learning_rate.assign(lr)

# Rolling window cross-validation - optimized
class FixedRollingWindowSplit:
    def __init__(self, train_size_samples, test_size_samples, step_size_samples):
        self.train_size = train_size_samples
        self.test_size = test_size_samples
        self.step_size = step_size_samples
        self.n_splits = None

    def split(self, X):
        n_samples = len(X)
        if self.train_size + self.test_size > n_samples:
            raise ValueError("Training and test size combined is larger than the dataset.")

        self.n_splits = (n_samples - self.train_size - self.test_size) // self.step_size + 1
        if self.n_splits < 1:
            raise ValueError("The specified sizes do not allow for a single fold to be created.")

        for i in range(self.n_splits):
            start = i * self.step_size
            
            if start + self.train_size + self.test_size > n_samples:
                break
            
            train_indices = np.arange(start, start + self.train_size)
            test_indices = np.arange(start + self.train_size, start + self.train_size + self.test_size)
            
            yield (train_indices, test_indices)

    def get_n_splits(self):
        return self.n_splits

def set_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)
    tf.random.set_seed(seed)

# Optimized data loading and preprocessing
def load_and_preprocess_data(filepath):
    # Use efficient data types
    data = pd.read_csv(filepath, parse_dates=['dates'], dayfirst=True)

    # Feature Engineering - vectorized operations
    data['days_since_start'] = (data['dates'] - data['dates'].min()).dt.days
    data['month'] = data['dates'].dt.month
    
    # Efficient rolling operations
    for col in ['Hs', 'Tp', 'Dir', 'Eng']:
        for window in [7, 30]:
            data[f"{col}_ma_{window}"] = data[col].rolling(window, min_periods=1).mean()
    
    # Vectorized cyclical features
    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)
    data['Dir_sin'] = np.sin(2 * np.pi * data['Dir'] / 360)
    data['Dir_cos'] = np.cos(2 * np.pi * data['Dir'] / 360)

    data = data.dropna()

    train = data[data['dates'].dt.year <= 2020]
    test = data[data['dates'].dt.year > 2020]

    features = ['Hs', 'Tp', 'Dir', 'Eng', 'month_sin', 'month_cos']
    target = 'shore'

    # Use float32 for better GPU performance
    scaler = StandardScaler()
    X_train = scaler.fit_transform(train[features]).astype(np.float32)
    X_test = scaler.transform(test[features]).astype(np.float32)

    y_scaler = StandardScaler()
    y_train = y_scaler.fit_transform(train[target].values.reshape(-1, 1)).flatten().astype(np.float32)
    y_test = y_scaler.transform(test[target].values.reshape(-1, 1)).flatten().astype(np.float32)

    train_dates = train['dates'].values
    test_dates = test['dates'].values

    return X_train, X_test, y_train, y_test, train_dates, test_dates, scaler, y_scaler

# Highly optimized sequence creation
@tf.function
def create_sequences_tf(X, y, window_size):
    """TensorFlow optimized sequence creation"""
    X = tf.convert_to_tensor(X, dtype=tf.float32)
    y = tf.convert_to_tensor(y, dtype=tf.float32)
    
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
    
    def extract_sequences(window):
        windowed = window.batch(window_size + 1)
        return windowed.map(lambda seq: (seq[:-1], seq[-1, 0]))  # Assuming y is first column
    
    dataset = dataset.flat_map(extract_sequences)
    return dataset

def create_sequences(X, y, dates, window_size=60):
    """Optimized sequence creation using stride_tricks"""
    n_samples = len(X) - window_size + 1
    
    if n_samples <= 0:
        return np.array([]), np.array([]), np.array([])
    
    # Use stride_tricks for efficient sequence creation
    shape = (n_samples, window_size, X.shape[1])
    strides = (X.strides[0], X.strides[0], X.strides[1])
    X_seq = np.lib.stride_tricks.as_strided(X, shape=shape, strides=strides).copy()
    
    y_seq = y[window_size-1:]
    date_seq = dates[window_size-1:]
    
    return X_seq.astype(np.float32), y_seq.astype(np.float32), date_seq

# Highly optimized tf.data pipeline to eliminate GPU starvation
def create_efficient_dataset(X, y, batch_size, shuffle=True, cache=True, num_parallel_calls=None):
    """Create aggressive data pipeline to maximize GPU utilization"""
    if num_parallel_calls is None:
        num_parallel_calls = tf.data.AUTOTUNE
    
    # Convert to TensorFlow tensors first for better performance
    X_tensor = tf.constant(X, dtype=tf.float32)
    y_tensor = tf.constant(y, dtype=tf.float32)
    
    dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))
    
    # Cache BEFORE any transformations for maximum speed
    if cache:
        dataset = dataset.cache()
    
    if shuffle:
        # Large shuffle buffer for better randomization
        shuffle_buffer = min(len(X), 50000)
        dataset = dataset.shuffle(
            buffer_size=shuffle_buffer, 
            reshuffle_each_iteration=True,
            seed=42
        )
    
    # Batch with parallel processing
    dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=num_parallel_calls)
    
    # Aggressive prefetching - this is key for preventing GPU starvation
    dataset = dataset.prefetch(buffer_size=10)
    
    # Additional optimization: repeat for continuous data flow during training
    if shuffle:  # Only repeat training datasets
        dataset = dataset.repeat()
    
    return dataset

# Optimized iTransformer model
def build_itransformer(input_shape, num_heads=4, key_dim=32, ff_dim=64, dropout=0.2, num_blocks=2):
    """Optimized iTransformer with better GPU utilization"""
    inputs = Input(shape=input_shape, dtype=tf.float32)
    x = Permute((2, 1))(inputs)  # (batch, features, time)
    
    for i in range(num_blocks):
        # Multi-head attention with residual connection
        attention_output = MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=key_dim, 
            dropout=dropout,
            use_bias=True
        )(x, x)
        x = Add()([x, attention_output])
        x = LayerNormalization(epsilon=1e-6)(x)
        
        # Feed-forward network with residual connection
        ff_output = Dense(ff_dim, activation='gelu', kernel_regularizer=l2(0.001))(x)
        ff_output = Dropout(dropout)(ff_output)
        ff_output = Dense(input_shape[0], kernel_regularizer=l2(0.001))(ff_output)
        x = Add()([x, ff_output])
        x = LayerNormalization(epsilon=1e-6)(x)
    
    x = Permute((2, 1))(x)  # Back to (batch, time, features)
    x = Flatten()(x)
    
    # Denser final layers for better GPU utilization
    x = Dense(64, activation='gelu', kernel_regularizer=l2(0.001))(x)
    x = Dropout(dropout)(x)
    x = Dense(32, activation='gelu', kernel_regularizer=l2(0.001))(x)
    x = Dropout(dropout)(x)
    
    outputs = Dense(1, dtype=tf.float32)(x)  # Ensure float32 output
    
    model = Model(inputs=inputs, outputs=outputs)
    return model

# Optimized Optuna objective with better GPU utilization
def objective_cv(trial, X_train_seq, y_train_seq, window_size=60):
    params = {
        'num_heads': trial.suggest_categorical('num_heads', [4, 6, 8]),
        'key_dim': trial.suggest_categorical('key_dim', [64, 128, 256]),
        'ff_dim': trial.suggest_categorical('ff_dim', [64, 128, 256]),
        'dropout': trial.suggest_float('dropout', 0.1, 0.4),
        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
        'num_blocks': trial.suggest_int('num_blocks', 1, 3),
        'batch_size': trial.suggest_categorical('batch_size', [28, 256, 512])  # Larger batch sizes
    }
    
    cv_scores = []
    
    rws_folding = FixedRollingWindowSplit(
        train_size_samples=10* 365,  # Smaller for faster training
        test_size_samples=3 * 365,
        step_size_samples=3 * 365
    )
    
    for fold_idx, (train_index, val_index) in enumerate(rws_folding.split(X_train_seq)):
        if fold_idx >= 2:  # Limit folds for faster optimization
            break
            
        X_train_fold, X_val_fold = X_train_seq[train_index], X_train_seq[val_index]
        y_train_fold, y_val_fold = y_train_seq[train_index], y_train_seq[val_index]
        
        if len(X_train_fold) == 0 or len(X_val_fold) == 0:
            continue

        # Create AGGRESSIVE datasets with maximum prefetching
        train_dataset = create_efficient_dataset(
            X_train_fold, y_train_fold, 
            batch_size=params['batch_size'], 
            shuffle=True,
            cache=True,
            num_parallel_calls=tf.data.AUTOTUNE
        )
        val_dataset = create_efficient_dataset(
            X_val_fold, y_val_fold, 
            batch_size=params['batch_size'], 
            shuffle=False,
            cache=True,
            num_parallel_calls=tf.data.AUTOTUNE
        )
        
        # Calculate steps per epoch for repeated dataset
        steps_per_epoch = max(1, len(X_train_fold) // params['batch_size'])
        validation_steps = max(1, len(X_val_fold) // params['batch_size'])
            
        model = build_itransformer(
            input_shape=(window_size, X_train_seq.shape[2]),
            num_heads=params['num_heads'],
            key_dim=params['key_dim'],
            ff_dim=params['ff_dim'],
            dropout=params['dropout'],
            num_blocks=params['num_blocks']
        )
        
        # Use mixed precision optimizer
        optimizer = Adam(learning_rate=params['learning_rate'])
        model.compile(
            optimizer=optimizer, 
            loss='mse',
            metrics=['mae']
        )
        
        early_stop = EarlyStopping(
            monitor='val_loss', 
            patience=5,  # Reduced for faster optimization
            restore_best_weights=True,
            min_delta=1e-6
        )
        
        # Train with datasets and explicit steps
        history = model.fit(
            train_dataset,
            steps_per_epoch=steps_per_epoch,
            validation_data=val_dataset,
            validation_steps=validation_steps,
            epochs=30,  # Reduced for optimization
            callbacks=[early_stop],
            verbose=0
        )
        
        # Evaluate
        val_loss = min(history.history['val_loss'])
        cv_scores.append(val_loss)
        
        # Clear memory
        del model, train_dataset, val_dataset
        tf.keras.backend.clear_session()
        
    return np.mean(cv_scores) if cv_scores else float('inf')

# Main execution with optimizations
def main():
    # Configure GPU first
    configure_gpu()
    set_seed(42)
    
    # Optimized parameters for MAXIMUM GPU utilization
    WINDOW_SIZE = 60
    EPOCHS = 200
    BATCH_SIZE = 256  # Even larger batch size for better GPU utilization
    WARMUP_EPOCHS = 20
    N_TRIALS = 10  # Reduced for faster execution

    print("Loading and preprocessing data...")
    X_train, X_test, y_train, y_test, train_dates, test_dates, scaler, y_scaler = load_and_preprocess_data(
        '/home/ubuntu/DeepLearning/otama_shore_wave.csv')

    print("Creating training sequences...")
    X_train_seq, y_train_seq, train_date_seq = create_sequences(X_train, y_train, train_dates, WINDOW_SIZE)
    print("Creating test sequences...")
    X_test_seq, y_test_seq, test_date_seq = create_sequences(X_test, y_test, test_dates, WINDOW_SIZE)
    
    print(f"Training sequences shape: {X_train_seq.shape}")
    print(f"Test sequences shape: {X_test_seq.shape}")

    # Optuna optimization with GPU monitoring
    print("Starting hyperparameter optimization...")
    study = optuna.create_study(direction='minimize', study_name='iTransformer_GPU_optimization')
    study.optimize(
        lambda trial: objective_cv(trial, X_train_seq, y_train_seq, WINDOW_SIZE), 
        n_trials=N_TRIALS, 
        show_progress_bar=True
    )

    best_params = study.best_params
    print("Best hyperparameters:", best_params)

    # Final model training with optimized parameters
    split_index = int(len(X_train_seq) * 0.85)  # Use more data for training
    X_train_final, X_val_final = X_train_seq[:split_index], X_train_seq[split_index:]
    y_train_final, y_val_final = y_train_seq[:split_index], y_train_seq[split_index:]

    # Create MAXIMALLY optimized datasets for final training
    print("Creating optimized data pipelines...")
    train_dataset = create_efficient_dataset(
        X_train_final, y_train_final, 
        batch_size=BATCH_SIZE, 
        shuffle=True,
        cache=True,
        num_parallel_calls=tf.data.AUTOTUNE
    )
    val_dataset = create_efficient_dataset(
        X_val_final, y_val_final, 
        batch_size=BATCH_SIZE, 
        shuffle=False,
        cache=True,
        num_parallel_calls=tf.data.AUTOTUNE
    )
    
    # Calculate steps for repeated datasets
    steps_per_epoch = max(1, len(X_train_final) // BATCH_SIZE)
    validation_steps = max(1, len(X_val_final) // BATCH_SIZE)

    # Build final model
    print("Building final model...")
    model = build_itransformer(
        input_shape=(WINDOW_SIZE, X_train_seq.shape[2]),
        num_heads=best_params.get('num_heads', 8),
        key_dim=best_params.get('key_dim', 128),
        ff_dim=best_params.get('ff_dim', 256),
        dropout=best_params.get('dropout', 0.2),
        num_blocks=best_params.get('num_blocks', 3)
    )
    
    initial_lr = best_params.get('learning_rate', 1e-3)
    optimizer = Adam(learning_rate=initial_lr)
    model.compile(
        optimizer=optimizer, 
        loss='mse',
        metrics=['mae']
    )

    # Callbacks for final training with GPU monitoring
    gpu_monitor = GPUMonitorCallback(log_frequency=50)
    early_stop = EarlyStopping(
        monitor='val_loss', 
        patience=25, 
        min_delta=1e-6, 
        restore_best_weights=True
    )
    checkpoint = ModelCheckpoint(
        'best_model.keras', 
        monitor='val_loss', 
        save_best_only=True, 
        verbose=1
    )
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', 
        factor=0.5, 
        patience=10, 
        min_lr=1e-7,
        verbose=1
    )

    print("Starting final training with aggressive GPU optimization...")
    print(f"Training on {len(X_train_final)} samples, validating on {len(X_val_final)} samples")
    print(f"Steps per epoch: {steps_per_epoch}, Validation steps: {validation_steps}")
    
    # Monitor GPU utilization during training with explicit steps
    history = model.fit(
        train_dataset,
        steps_per_epoch=steps_per_epoch,
        epochs=EPOCHS,
        validation_data=val_dataset,
        validation_steps=validation_steps,
        callbacks=[early_stop, checkpoint, reduce_lr, gpu_monitor],
        verbose=1
    )

    # Load best model and make predictions
    print("Loading best model and making predictions...")
    model = tf.keras.models.load_model('best_model.keras')
    
    # Predict in batches for better GPU utilization
    train_pred = model.predict(X_train_seq, batch_size=BATCH_SIZE, verbose=1)
    test_pred = model.predict(X_test_seq, batch_size=BATCH_SIZE, verbose=1)

    # Inverse transform predictions
    train_pred_inv = y_scaler.inverse_transform(train_pred)
    test_pred_inv = y_scaler.inverse_transform(test_pred)
    y_train_seq_inv = y_scaler.inverse_transform(y_train_seq.reshape(-1, 1))
    y_test_seq_inv = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1))

    # Evaluation metrics
    print("\nFinal Evaluation Metrics:")
    print(f"Train MAE: {mean_absolute_error(y_train_seq_inv, train_pred_inv):.4f}")
    print(f"Train RMSE: {np.sqrt(mean_squared_error(y_train_seq_inv, train_pred_inv)):.4f}")
    print(f"Train R²: {r2_score(y_train_seq_inv, train_pred_inv):.4f}")
    print(f"Test MAE: {mean_absolute_error(y_test_seq_inv, test_pred_inv):.4f}")
    print(f"Test RMSE: {np.sqrt(mean_squared_error(y_test_seq_inv, test_pred_inv)):.4f}")
    print(f"Test R²: {r2_score(y_test_seq_inv, test_pred_inv):.4f}")

    # Plotting results
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)
    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
    plt.title('Model Training Progress', fontsize=14)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('training_loss.png', dpi=300, bbox_inches='tight')
    plt.show()

    plt.figure(figsize=(16, 8))
    plt.plot(test_date_seq, y_test_seq_inv, label='Actual', color='blue', linewidth=2, alpha=0.8)
    plt.plot(test_date_seq, test_pred_inv, label='Predicted', color='red', linewidth=2, alpha=0.8)
    plt.title('Shoreline Position Prediction - Test Set', fontsize=14)
    plt.xlabel('Date', fontsize=12)
    plt.ylabel('Shoreline Position', fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    plt.gcf().autofmt_xdate()
    plt.tight_layout()
    plt.savefig('shoreline_prediction_test.png', dpi=300, bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    main()
